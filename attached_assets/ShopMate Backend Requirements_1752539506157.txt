ShopMate Backend Requirements Document
1. Introduction
This document outlines the specific backend requirements for the "ShopMate" application. It details the server-side architecture, API endpoints, database design, core business logic, and technical considerations necessary to support the frontend functionalities and achieve the overall project goals. This project emphasizes demonstrating MERN stack capabilities while adhering to ethical considerations for web scraping.

2. Overall Backend Principles
Scalability: Design for horizontal scaling to handle increasing user load and data volume.

Security: Implement robust security measures to protect user data and prevent unauthorized access.

Performance: Optimize API response times and data processing for a fast user experience.

Reliability: Ensure high availability and fault tolerance, with effective error handling and logging.

Maintainability: Develop modular, well-documented, and testable code for easy updates and extensions.

Data Integrity: Ensure consistency and accuracy of all stored data.

3. Technical Stack
Runtime Environment: Node.js

Web Framework: Express.js

Database: MongoDB (NoSQL Document Database)

ORM/ODM (Optional but Recommended): Mongoose.js for MongoDB object modeling.

SERP API Integration: For primary product search results (Google Shopping Engine).

Web Scraping: Puppeteer, Cheerio (specifically for Daily Deals getting and Order Tracking status updates, and potentially for fetching deeper product details if not available via SERP API).

Authentication: JSON Web Tokens (JWT).

Data Validation: Joi, Express-validator (or similar).

Task Scheduling: Node-cron (or similar for scheduled jobs).

Logging: Winston, Morgan (or similar).

4. API Endpoints
All APIs will be RESTful, returning JSON responses. Standard HTTP status codes will be used.

4.1. Authentication & User Management
POST /api/auth/register

Purpose: Register a new user.

Request Body: { "email": "string", "password": "string" }

Response: { "token": "string", "userId": "string", "email": "string" } (JWT token, user ID)

Validation: Email format, password strength.

POST /api/auth/login

Purpose: Authenticate an existing user.

Request Body: { "email": "string", "password": "string" }

Response: { "token": "string", "userId": "string", "email": "string" }

Validation: Correct credentials.

GET /api/auth/me (Protected)

Purpose: Get current authenticated user's profile.

Response: { "userId": "string", "email": "string", "preferences": { ... } }

PUT /api/users/preferences (Protected)

Purpose: Update user preferences (theme).

Request Body: { "theme": "light" | "dark" }

Response: { "message": "Preferences updated successfully." }

4.2. Product Search & Comparison
GET /api/products/search

Purpose: Search for products across various platforms via Google Shopping Engine.

Query Params: q (search query, required), sort (optional, "price_asc" or "price_desc"), platform (optional, e.g., "meesho", "ajio" - comma-separated for multiple, for filtering SERP results).

Response: [ { "id": "string", "name": "string", "imageUrl": "string", "lowestPrice": number, "bestValue": boolean, "platforms": [ { "name": "string", "logoUrl": "string", "price": number, "discount": number, "inclusivePrice": number, "deliveryDate": "date string", "productUrl": "string", "sellerRating": number } ], "priceHistory": [ { "date": "date string", "price": number } ] } ] (Array of aggregated product objects)

Logic:

Receive search query.

SERP API Call: Send the search query (q) to SERP API (Google Shopping Engine) to retrieve product results.

Data Extraction & Normalization: Extract the required data points from the SERP API response for each product.

Data Points to Extract:

Product Image (URL)

Product Title

Product Stars (rating, e.g., 4.5)

Reviews Count

Product Brand

Product Price

Product Key Features (e.g., bullet points)

Product Specifications (e.g., technical details in key-value pairs)

Product Description

Handling Unavailable Data: If any of the above fields are unavailable from the SERP API response, the respective field in the stored data should be explicitly set to a value like "unavailable".

Aggregate results, normalize data (e.g., currency, delivery date formats).

Calculate "inclusive price" (product price + estimated shipping + estimated taxes).

Determine "best value" based on price, inclusive price, and potentially delivery speed/seller rating.

Data Storage: Store/update cached product data and price history in the MongoDB database for persistence, comparison, and price trend display. Client-side storage (session/cookie) is not suitable for this aggregated, multi-platform data required for core features.

Price Update Logic: When new data is received from SERP API for an existing product:

Compare the newly obtained currentPrice (for each platform/merchant) with the currentPrice already stored in the database for that specific platform/merchant and product.

If a price change is detected, update the lastPriceChangeAt field for the product with the current timestamp.

Always add the new price point to the priceHistory array for that product and platform/merchant.

Apply sorting by price (ascending or descending) and filtering by platform(s) (if SERP API provides merchant data that can be mapped to the specified platforms like Meesho, Ajio, Croma, Reliance Digital, Big Basket, Amazon, Flipkart, Myntra) as specified in the query parameters. Other sorting/filtering criteria are not supported for this project.

Return aggregated and sorted results.

GET /api/products/:id

Purpose: Get detailed information and insights for a specific product.

Response: { "id": "string", "name": "string", "description": "string", "imageUrls": ["string"], "platforms": [ { "name": "string", "logoUrl": "string", "price": number, "discount": number, "inclusivePrice": number, "deliveryDate": "date string", "productUrl": "string", "sellerRating": number } ], "priceHistory": [ { "date": "date string", "price": number } ], "overallRating": number, "reviewCount": number, "lastPriceChangeAt": "date string" (timestamp of last price change) }

Logic:

First, attempt to fetch detailed product data and insights from the cached data in MongoDB (which originated from SERP API).

If the product data is not found in the database, then query the SERP API again using the product ID or relevant identifiers to retrieve the information.

If necessary, perform a targeted scrape of the original product URL (if provided by SERP API and not fully detailed in cache) to get missing details.

Return the product information as a JSON object, whether it originated from the database cache or a fresh SERP API query.

4.3. Wishlist Management
GET /api/wishlist (Protected)

Purpose: Get the authenticated user's wishlist.

Response: [ { "productId": "string", "name": "string", "imageUrl": "string", "currentLowestPrice": number, "targetPrice": number (optional), "platforms": [ { ... } ] } ]

POST /api/wishlist (Protected)

Purpose: Add a product to the user's wishlist.

Request Body: { "productId": "string", "productUrl": "string", "platform": "string" } (or more detailed product info if not already in DB)

Response: { "message": "Product added to wishlist." }

DELETE /api/wishlist/:productId (Protected)

Purpose: Remove a product from the user's wishlist.

Response: { "message": "Product removed from wishlist." }

4.4. Order Tracking (Manual with On-Demand Scraping)
GET /api/orders (Protected)

Purpose: Get the authenticated user's tracked orders.

Response: [ { "orderId": "string", "productName": "string", "platform": "string", "purchaseDate": "date string", "status": "string", "orderUrl": "string" } ]

POST /api/orders (Protected)

Purpose: Add a new order to track (manual entry).

Request Body: { "orderId": "string", "productName": "string", "platform": "string", "purchaseDate": "date string", "orderUrl": "string" }

Logic:

Receive order details and the orderUrl from the frontend.

Initiate a scrape of the provided orderUrl to fetch the initial order status and any other relevant publicly available details from that specific page.

Store the order details, including the scraped status, in the MongoDB orders collection.

Caution: Scraping private order pages can be highly sensitive. For this educational project, assume the orderUrl points to a page where status is publicly accessible or that the user has explicitly provided a URL they are comfortable with being accessed by the application for demonstration purposes. In a real-world scenario, this would strictly require official APIs and explicit user consent. Order tracking can be done for different e-commerce sites as long as their policies allow for such tracking (e.g., publicly accessible status pages).

Response: { "message": "Order added for tracking.", "order": { ...newly_added_order_details_including_scraped_status... } }

PUT /api/orders/:orderId/refresh-status (Protected)

Purpose: Manually trigger an update of a tracked order's status.

Request Body: None (order ID from URL param)

Logic:

Retrieve the orderUrl associated with :orderId from the database.

Triggered by User Action: This scrape will only be initiated when the user explicitly clicks a "Check Update Status" button on the frontend.

Perform a scrape of the orderUrl to fetch the latest status.

Update the status field for the corresponding order in the MongoDB orders collection.

Response: { "message": "Order status updated.", "newStatus": "string" }

DELETE /api/orders/:orderId (Protected)

Purpose: Remove a tracked order.

Response: { "message": "Order removed." }

4.5. Search History
POST /api/search/history

Purpose: Record a user's search query for history and analytics.

Request Body: { "query": "string", "productId": "string" (optional, if a specific product was clicked from results) }

Headers: Authorization: Bearer <token> (for authenticated users) or Cookie: sessionId=<value> (for non-authenticated users).

Logic:

Extract query and optional productId.

Exclude Common Searches: Before storing, check if the query is a common, collective noun (e.g., "smartphones", "laptops", "dresses"). If it is, do not store this query in the searchHistory collection.

If Authorization header is present, link to userId.

If Authorization header is not present, attempt to retrieve sessionId from the Cookie header. If no sessionId is found, generate a new one and instruct the frontend (via Set-Cookie header in response or a separate mechanism) to set it.

Store the query, timestamp, associated userId (if authenticated) or sessionId (if non-authenticated), and productId (if provided) in the searchHistory collection.

Response: { "message": "Search query recorded.", "sessionId": "string" (if a new session was created) }

GET /api/search/history (Protected or Session-based)

Purpose: Retrieve a user's (or session's) search history.

Headers: Authorization: Bearer <token> (for authenticated users) or Cookie: sessionId=<value> (for non-authenticated users).

Logic:

If Authorization header is present, fetch history for the authenticated userId.

If Authorization header is not present, fetch history for the provided sessionId from the Cookie header.

Return a list of recent search queries, potentially with timestamps and linked product details.

Response: [ { "query": "string", "timestamp": "date string", "productId": "string" (optional), "productName": "string" (optional, from products collection) } ]

4.6. Daily Deals
GET /api/deals (Public)

Purpose: Retrieve daily deals from various e-commerce platforms.

Query Params: None.

Response: [ { "id": "string", "name": "string", "imageUrl": "string", "originalPrice": number, "dealPrice": number, "discountPercentage": number, "platform": "string", "productUrl": "string", "scrapedAt": "date string" } ] (Array of deal product objects)

Logic:

Triggered by User Action: This endpoint will initiate direct web scraping for daily deals only when a user explicitly clicks an "update today's deals" button on the frontend.

Perform targeted scraping of "daily deals" or "top offers" sections from specified e-commerce platforms (Meesho, Ajio, Croma, Reliance Digital, Big Basket, Amazon, Flipkart, Myntra, etc., focusing on those where deals are scrapable).

Data Storage: Store the scraped deals in the new deals collection in MongoDB. This ensures that subsequent requests within a reasonable timeframe can serve cached data without re-scraping immediately.

Apply anti-detection tactics and rate limiting as defined in Section 6.2.

Return the scraped and stored daily deals.

5. Database Requirements (MongoDB)
5.1. Collections & Schemas
users Collection:

_id: ObjectId (User ID)

email: String (Unique, Indexed)

passwordHash: String (Hashed password)

preferences: Object

theme: String ("light", "dark", default "light")

createdAt: Date

updatedAt: Date

products Collection:

_id: ObjectId (Product ID, or generated from unique product identifier + platform)

name: String (Product Title)

description: String (Product Description, Optional, might be fetched on demand)

imageUrl: String (Product Image URL)

category: String

brand: String (Product Brand)

productStars: Number (e.g., 4.5, or "unavailable")

reviewsCount: Number (or "unavailable")

keyFeatures: [String] (Product Key Features, or ["unavailable"])

specifications: Object (Product Specifications, key-value pairs, or {"status": "unavailable"})

searchKeywords: [String] (For internal search optimization)

platforms: [Object] (Array of platform-specific data)

name: String (e.g., "Amazon", "Flipkart", "Meesho", "Ajio", "Croma", "Reliance Digital", "Big Basket" - based on SERP API merchant data)

logoUrl: String

productUrl: String (Link to original product page)

currentPrice: Number (Product Price)

discount: Number (percentage)

inclusivePrice: Number (Price + shipping + taxes)

deliveryDate: Date (Estimated)

sellerRating: Number (Optional)

priceHistory: [Object] (Array of historical prices)

date: Date

price: Number

lastScrapedAt: Date (Timestamp of last data update)

lastPriceChangeAt: Date (New field: Timestamp of the last detected price change)

createdAt: Date

updatedAt: Date

wishlists Collection:

_id: ObjectId

userId: ObjectId (Reference to users collection, Indexed)

productId: ObjectId (Reference to products collection, Indexed)

addedAt: Date

targetPrice: Number (Optional, for user-defined alerts)

orders Collection:

_id: ObjectId

userId: ObjectId (Reference to users collection, Indexed)

orderId: String (Original order ID from platform)

productName: String

platform: String

purchaseDate: Date

status: String (e.g., "Pending", "Shipped", "Delivered", "Cancelled")

orderUrl: String (Link to original order page)

createdAt: Date

updatedAt: Date

lastStatusCheckAt: Date (Timestamp of last status scrape)

searchHistory Collection:

_id: ObjectId

query: String

timestamp: Date

userId: ObjectId (Optional, reference to users collection)

sessionId: String (Optional, for non-authenticated users)

productId: ObjectId (Optional, reference to products collection, if a specific product was clicked from results)

deals Collection (New):

_id: ObjectId

name: String (Deal Product Title)

imageUrl: String (Deal Product Image URL)

originalPrice: Number

dealPrice: Number

discountPercentage: Number

platform: String (e.g., "Meesho", "Ajio")

productUrl: String (Link to the deal product page)

scrapedAt: Date (Timestamp when this deal was last scraped)

createdAt: Date

updatedAt: Date

5.2. Indexing Strategy
users: email (unique), _id

products: name, category, brand, platforms.name, lastScrapedAt, lastPriceChangeAt

wishlists: userId, productId (compound index userId_productId for uniqueness)

orders: userId, orderId

searchHistory: userId, sessionId, timestamp (for efficient retrieval)

deals: platform, scrapedAt (for efficient retrieval and caching logic)

6. Core Backend Modules/Services
6.1. Authentication & Authorization Module
User Registration: Hash passwords using bcrypt.

User Login: Compare hashed passwords. Generate and sign JWTs.

Middleware: Protect routes using JWT verification.

6.2. Product Data Aggregation Service
SERP API Integration Logic:

Utilize a SERP API client library (if available for Node.js) or direct fetch/axios calls to interact with the Google Shopping Engine API.

Handle API key management and rate limits for SERP API.

Parse and normalize the JSON response from SERP API into the products schema format.

Web Scraping Logic (for Daily Deals & Order Tracking & Supplemental Details):

Use Puppeteer for dynamic content and browser automation.

Use Cheerio for efficient DOM parsing once HTML is obtained.

Platform-Specific Scrapers: Develop individual scraper functions for specific e-commerce platforms for:

Daily Deals Getting: Targeted scraping of "daily deals" or "top offers" sections on platforms like Meesho, Ajio, Croma, Reliance Digital, Big Basket, Amazon, Flipkart, Myntra, focusing on those where deals are readily scrapable.

Order Tracking: On-demand status updates for manually added orders.

Supplemental Product Details: If SERP API lacks specific, crucial product details (e.g., very detailed specifications not in Google Shopping results), a targeted scrape of the original product URL might be performed.

Error Handling: Graceful handling of scraper failures (e.g., site structure changes, CAPTCHAs).

Rate Limiting & Anti-Detection: Implement strategies to avoid IP blocking (e.g., using proxy services, random delays between requests, rotating user agents). These tactics and the minimum of 15 scrapes per day per platform apply to all direct web scraping activities (Daily Deals, Order Tracking, and any Supplemental Product Details scraping), not the main product search via SERP API.

Data Normalization: Standardize product data from various sources (SERP API, direct scrapes) into a consistent schema, ensuring "unavailable" is used for missing fields.

Caching Strategy:

Store product data (from SERP API and direct scrapes) in the products collection.

Implement a TTL (Time-To-Live) or a refresh mechanism for cached data (e.g., re-query SERP API or re-scrape if data is older than X hours/days).

Prioritize cached data for initial display, then refresh in background.

6.3. Price Comparison & Best Value Logic
Inclusive Price Calculation:

Fetch base price, shipping cost (if available/estimable), and estimated taxes.

inclusivePrice = basePrice + shippingCost + taxes.

Best Value Determination:

Algorithm considering inclusivePrice as primary factor.

Secondary factors: deliveryDate, sellerRating, discountPercentage.

Could be a simple weighted average or a more complex scoring system.

6.4. Wishlist Service
Price Monitoring: Scheduled background jobs to periodically check prices of items in user wishlists (using cached products data).

Deal Detection: Compare current prices with historical prices or target prices to identify drops.

6.5. Order Tracking Service (Manual with On-Demand Scraping)
Initial Scrape on Add: When a user adds an order, trigger a scrape of the provided orderUrl to get the initial status.

On-Demand Status Refresh: When the user explicitly requests an update for a specific order, trigger a scrape of its orderUrl to fetch the latest status.

Data Storage: Store order details and status in the orders collection.

6.6. Search History Service
Record Search: Store search queries with timestamps, linking to userId for authenticated users or sessionId for non-authenticated users.

Session Management: Generate and manage sessionId cookies for non-authenticated users to maintain their search history across sessions.

Retrieve History: Fetch and return relevant search history based on user authentication status.

6.7. Daily Deals Service (New)
On-Demand Scraping: When the /api/deals endpoint is hit, initiate direct web scraping for daily deals from configured e-commerce platforms.

Data Storage: Store the scraped deal data in the deals collection.

Caching for Deals: Implement a caching mechanism for deals (e.g., only scrape if the last scrape was more than X minutes/hours ago) to avoid redundant requests.

7. Non-Functional Backend Requirements
7.1. Performance:

API Response Time: Aim for average API response times under 500ms for authenticated requests, and under 2 seconds for complex search queries (including SERP API call time). For direct scraping endpoints (/api/deals, order tracking refresh), response times might be higher due to network latency and scraping process, but should be optimized.

Database Query Optimization: Utilize appropriate indexes.

Caching: Implement server-side caching for frequently accessed data (e.g., popular product searches, daily deals).

7.2. Scalability:

Stateless APIs: Design APIs to be stateless to facilitate horizontal scaling.

Load Balancing: Support deployment behind a load balancer.

Microservices (Future Consideration): For very large scale, consider breaking down services (e.g., separate scraping service).

7.3. Security:

Input Validation: Strict validation for all incoming request data to prevent injection attacks.

Password Hashing: Use strong, modern hashing algorithms (e.g., bcrypt) for passwords.

JWT Security: Securely store JWTs (e.g., in HTTP-only cookies), set appropriate expiration times.

CORS: Properly configure Cross-Origin Resource Sharing.

Environment Variables: Store sensitive information (API keys, database credentials) as environment variables, not hardcoded.

HTTPS: Enforce HTTPS for all API communication.

7.4. Reliability:

Error Handling: Implement centralized error handling middleware.

Retry Mechanisms: For external API calls (SERP API, direct scraping), implement retry logic with exponential backoff.

Graceful Degradation: If SERP API or a specific platform scraper fails, ensure the application still functions gracefully (e.g., by returning partial results or an informative error).

7.5. Maintainability:

Modular Codebase: Organize code into logical modules (e.g., routes, controllers, services, models).

Code Comments & Documentation: Thoroughly comment code and document API endpoints (e.g., using Swagger/OpenAPI).

Automated Testing: Implement unit and integration tests for backend logic and API endpoints.

8. Error Handling & Logging
Centralized Error Handling: Use Express middleware to catch and format errors, sending appropriate HTTP status codes and user-friendly messages.

Logging: Implement a robust logging system (e.g., Winston) to record:

API requests and responses.

Errors (with stack traces).

SERP API call successes/failures.

Direct scraping successes/failures.

Scheduled job executions.

Security events (e.g., failed login attempts).

Monitoring: Integrate with monitoring tools (e.g., Prometheus, Grafana, cloud provider monitoring) to track backend health and performance metrics.

9. Deployment Considerations
Environment Variables: Utilize .env files or cloud-specific configuration management for sensitive data and environment-specific settings.

Containerization (Optional): Dockerize the Node.js application for easier deployment and portability.

Process Management: Use PM2 or similar tools for keeping Node.js applications alive in production.

Database Backup & Recovery: Implement a strategy for regular MongoDB backups.

This updated Backend Requirements Document provides a clear roadmap for your educational ShopMate project, balancing functionality with ethical considerations and showcasing your MERN stack development skills.